{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b4c0ae",
   "metadata": {},
   "source": [
    "<h1>CS4619: Artificial Intelligence II</h1>\n",
    "<h1>Transformers</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfed28a",
   "metadata": {},
   "source": [
    "<h1>Acknowledgements</h1>\n",
    "<ul>\n",
    "    <li>A few of the examples owe a debt to Chollet's book (2nd edition). The table also comes from Chollet's book.\n",
    "    </li>\n",
    "    <h1>Acknowledgements</h1>\n",
    "<ul>\n",
    "    <li>The colourful diagrams are my own invention but were improved by seeing similar diagrams in materials produced by Sebastian Raschka.</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb62a8c",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>\n",
    "<ul>\n",
    "    <li>We have learned a little about recurrent neural networks (RNNs) and their applications in natural language processing (NLP).</li>\n",
    "    <li>There have been many improvements to RNNs, such as beam search, bidirectional RNNs and attention mechanisms.</li>\n",
    "    <li>But a new architecture has emerged that outperforms RNNs: the <b>transformer</b> architecture.\n",
    "        <ul>\n",
    "            <li>Ashish Vaswani et al: <i>Attention is all you need.</i> In Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017. (<a href=\"https://arxiv.org/abs/1706.03762\">https://arxiv.org/abs/1706.03762</a>) \n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Transformers use <b>self-attention</b> (see below).</li>\n",
    "    <li>They do not contain any mechanisms for processing input sequentially: no recurrent layers, no 1D-convolutional layers, &hellip; They receive the input sequence all in one go.</li>\n",
    "    <li>Many of the language models that we reviewed at the end of the previous lecture do in fact use the transformer architecture.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3f149",
   "metadata": {},
   "source": [
    "<h1>Self-Attention</h1>\n",
    "<ul>\n",
    "    <li>The idea of self-attention is to model the dependencies between elements of the input sequence.\n",
    "        <ul>\n",
    "            <li>Indeed, the self-attention mechanism enables a transformer to capture <em>long-range</em> dependencies between elements of the input sequence.</li>\n",
    "            <li>Recall that this was something that caused difficulties for RNNs. Even using LSTMs and GRUs, which were developed to help overcome these difficulties, the problem still remained. \n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Consider a word such as ``date''. We have a single word embedding for this word: a vector of numbers. But this word is ambiguous. Its meaning is context-specific: other words that surround it affect its meaning. Consider:\n",
    "        <ul>\n",
    "            <li>``I marked the date of the seminar in my calendar.''</li>\n",
    "            <li>``I enjoyed our date together.''</li>\n",
    "            <li>``The date seller was not at the market today.''</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The idea in self-attention (roughly speaking) is to modify the word embedding based on the word embeddings of the surrounding words.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642713e5",
   "metadata": {},
   "source": [
    "<h2>A Simple Form of Self-Attention</h2>\n",
    "<ul>\n",
    "    <li>We start with a simple form of self-attention. Then later we make it more complicated.</li>\n",
    "    <li>Assume we have an input sentence and we compute the word embedding for each word in that input sentence: $\\v{x}_{(1)}, \\v{x}_{(2)}, \\ldots, \\v{x}_{(\\mathit{max\\_length})}$. Below, for simplicity, I sometimes say ``word'' when I mean ``word embedding''.\n",
    "    <li>In overview, we do the following for each word $\\v{x}_{(i)}$ in the input sequence:\n",
    "        <ul>\n",
    "            <li>Compute attention weights $\\alpha_{ij}$: simply, each weight is the similarity (dot product) between word $\\v{x}_{(i)}$ and every word $\\v{x}_{(j)}$ for $j=1\\ldots\\mathit{max\\_length}$ in the input sequence.\n",
    "                $$\\alpha_{ij} = \\v{x}_{(i)}\\v{x}_{(j)}$$\n",
    "                (So, if you like, $\\alpha_{ij}$ is the relevance of word $j$ to word $i$.)<br />\n",
    "                Note that these 'weights' are calculated; they are not learned. \n",
    "            </li>\n",
    "            <li>Normalise the weights: for this, we use softmax.</li>\n",
    "            <li>Compute a context-aware embedding $\\v{z}_{(i)}$ for $\\v{x}_{(i)}$: the weighted sum of all the word embeddings in the input sequence.\n",
    "                $$\\v{z}_{(i)} = \\sum_{j=1}^{\\mathit{max\\_length}} \\alpha_{ij}\\v{x}_{(j)}$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e138d53",
   "metadata": {},
   "source": [
    "<h3>Example</h3>\n",
    "<ul>\n",
    "    <li>Suppose the input sequence is ``the train left the station on time''.</li>\n",
    "    <li>We get the word embeddings for each word.</li>\n",
    "    <li>Suppose the current word is word 5: ``station''.</li>\n",
    "    <li>We compute the similarities (dot products) between the word embedding for ``station'' and the word embeddings for all the words in the input sentence. Suppose this gives the following:\n",
    "        $$\\cv{0.2\\\\0.8\\\\0.6\\\\0.2\\\\1.0\\\\0.2\\\\0.3}$$\n",
    "        So the similarity between ``train'' and ``station'', for example, is 0.8. Why do we have a 1.0 in there?\n",
    "    </li>\n",
    "    <li>Then we normalize using softmax to obtain (roughly):\n",
    "        $$\\cv{0.10\\\\0.19\\\\0.15\\\\0.10\\\\0.23\\\\0.10\\\\0.11}$$\n",
    "    </li>\n",
    "    <li>Finally, the context-aware embedding for ``station'' is the weighted sum of the word embeddings:\n",
    "        $$\\v{z}_{(5)} = \\mathit{sum}\\left(\\cv{\n",
    "        0.10 \\times \\v{x}_{(1)}\\\\\n",
    "        0.19 \\times \\v{x}_{(2)}\\\\\n",
    "        0.15 \\times \\v{x}_{(3)}\\\\\n",
    "        0.10 \\times \\v{x}_{(4)}\\\\\n",
    "        0.23 \\times \\v{x}_{(5)}\\\\\n",
    "        0.10 \\times \\v{x}_{(6)}\\\\\n",
    "        0.11 \\times \\v{x}_{(7)}}\\right)$$\n",
    "    </li>\n",
    "    <li>You can see that now we have an embedding for ``station'' that has been modified by its context, i.e. by the other words in the sentence.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c0934",
   "metadata": {},
   "source": [
    "<h3>Vectorization</h3>\n",
    "<ul>\n",
    "    <li>Above, we saw the calculations for one of the words (``station'').</li>\n",
    "    <li>But, we don't need to do this one word at a time.</li>\n",
    "    <li>We can vectorize:\n",
    "        <ul>\n",
    "            <li>Put the word embeddings into a matrix $\\v{X}$. E.g. if there are 7 words and the word embedding dimension is 100, then $\\v{X}$ is a $7 \\times 100$ matrix.\n",
    "            <li>Compute the similarities $\\v{A}$ between all pairs of words by multiplying $\\v{X}$ with itself (acutally with its transpose). In the example, this would give a $7 \\times 7$ matrix.</li>\n",
    "            <li>Normalize each column of the similarity matrix $\\v{A}$ using softmax.</li>\n",
    "            <li>Compute the context-aware vectors for all the words in the input sentence using a matrix multiplication between the normalized similarity matrix and a matrix of the word embeddings:\n",
    "                $$\\v{Z} = \\v{A}\\v{X}$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc6498",
   "metadata": {},
   "source": [
    "<img src=\"images/self_attention1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a3a8a0",
   "metadata": {},
   "source": [
    "<h2>A More Advanced Form of Self-Attention</h2>\n",
    "<ul>\n",
    "    <li>The simple form of self-attention that we studied above does not have any learnable parameters.</li>\n",
    "    <li>We introduce three additional weight matrices, $\\v{U}_q$, $\\v{U}_k$ and $\\v{U}_v$.\n",
    "        <ul>\n",
    "            <li>They are initialized at random, and then modified during back-prop.<br />\n",
    "                Note that these weights are learned &mdash; unlike the ones in $\\v{A}$.</li>\n",
    "            <li>Let the embedding dimension of the word embeddings be $d$. Then, it is common for $\\v{U}_q$, $\\v{U}_k$ and $\\v{U}_v$ to be $d \\times d$ matrices. And this is what we will assume in our presentation here.</li>\n",
    "            <li>(Advanced, ignore: more generally, we can choose values $d_k$ and $d_v$ and then $\\v{U}_q$ and $\\v{U}_k$ can be $d_k \\times d$, and $\\v{U}_v$ can be $d_v \\times d$.)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>How do we use these weight matrices?\n",
    "        <ul>\n",
    "            <li>We multiply the word embeddings $\\v{x}_{(i)}$ by each of the matrices:\n",
    "                $$\\v{q}_{(i)} = \\v{U}_q\\v{x}_{(i)}$$\n",
    "                $$\\v{k}_{(i)} = \\v{U}_k\\v{x}_{(i)}$$\n",
    "                $$\\v{v}_{(i)} = \\v{U}_v\\v{x}_{(i)}$$\n",
    "            </li>\n",
    "            <li>Previously, we computed the weights as the similarity (dot product) of the word embeddings: $\\alpha_{ij} = \\v{x}_{(i)}\\v{x}_{(j)}$. But, now:\n",
    "                $$\\alpha_{ij} = \\v{q}_{(i)}\\v{k}_{(j)}$$\n",
    "            </li>\n",
    "            <li>Then, we normalise the weights, as before: softmax. (Advanced, ignore: In fact, in transformers, we scale the weights by multiplying them by $1/\\sqrt{d}$ before we take the softmax. This gives a 'smoother' softmax distrbution.)</li>\n",
    "            <li>Finally, we compute a context-aware embedding. Previously, we computed a weighted sum: $\\v{z}_{(i)} = \\sum_{j=1}^{\\mathit{max\\_length}} \\alpha_{ij}\\v{x}_{(j)}$. But, now:\n",
    "              $$\\v{z}_{(i)} = \\sum_{j=1}^{\\mathit{max\\_length}} \\alpha_{ij}\\v{v}_{(j)}$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9159e",
   "metadata": {},
   "source": [
    "<img src=\"images/self_attention2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07926628",
   "metadata": {},
   "source": [
    "<h3>Query, key and value</h3>\n",
    "<ul>\n",
    "    <li>For each word, we computed three vectors, $\\v{q}_{(i)}$, $\\v{k}_{(i)}$ and $\\v{v}_{(i)}$.</li>\n",
    "    <li>People refer to these three as the <b>query</b>, <b>key</b> and <b>value</b> vectors.</li>\n",
    "    <li>Why? The inventors of transformers needed three names. They hijacked three names from the field of non-relational databases. In a non-relational database, we enter a query, it is matched against a key and a value is retrieved. If we are being charitable, we can see that in transformers query vectors are matched with key vectors when we compute similarities, and value vectors are part of the final result. So there is a tenuous connection.</li>\n",
    "     <li>For me, these names are unhelpful. They might just as well have called them Huey, Dewey and Louie in honour of Donald Duck's nephews!\n",
    "    </li>\n",
    "    <li>(Advanced, ignore: Above, we used $\\v{x}_{(i)}$ for calculating the query, key and value. This is what we do for sequence classification, e.g. for sentiment analysis. But, in fact, you could use a different vector for each. For example, in machine translation between English and French, a training example will have both an English sentence and a French sentence. You might use French words when computing the query, and English words when computing the keys and values. If you do this, then you are no longer really doing <em>self</em>-attention.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3d632",
   "metadata": {},
   "source": [
    "<h2>Multi-Head Attention</h2>\n",
    "<ul>\n",
    "    <li>Let's refer to the three matrices $\\v{U}_q$, $\\v{U}_k$ and $\\v{U}_v$ as a <b>head</b>. Then, what we have described is a transformer that has a single head.</li>\n",
    "    <li>But transformers use multi-head attention. Guess what: we have multiple heads. In other words, if we have $h$ heads then we have $h$ sets of these matrices:\n",
    "        <ul>\n",
    "            <li>$\\v{U}_{q_1}$, $\\v{U}_{k_1}$, $\\v{U}_{v_1}$</li>\n",
    "            <li>$\\v{U}_{q_2}$, $\\v{U}_{k_2}$, $\\v{U}_{v_2}$</li>\n",
    "            <li>&hellip;</li>\n",
    "            <li>$\\v{U}_{q_h}$, $\\v{U}_{k_h}$, $\\v{U}_{v_h}$</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Now, instead of computing one query vector, key vector and value vector per word, we compute $h$ of them.</li>\n",
    "    <li>Why do we want multiple heads? It gives the transformer more parameters and an opportunity to learn $h$ different representations. It's a bit like why we have multiple feature maps in a convolutional layer.\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/attention_examples.png\" />\n",
    "            <figcaption>Evidence that different heads learn to perform different tasks.<br />\n",
    "                Taken from the <i>Attention is all you need</i> paper\n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>(Advanced, ignore: In fact, instead of having $h$ sets of matrices, we combine them into just 3 matrices and use Boolean masks to access regions of the matrices. This improves efficiency: it means we can use just one matrix multiplication instead of $h$ matrix multiplications.)</li>\n",
    "    <li>The outputs of the multiple heads are concatenated.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa40ab4",
   "metadata": {},
   "source": [
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"images/multi_head_attention.png\" />\n",
    "    <figcaption>Taken from the <i>Attention is all you need</i> paper<br />\n",
    "        (Linear = Dense layers)\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07fccbb",
   "metadata": {},
   "source": [
    "<h2>Positional Encoding</h2>\n",
    "<h3>Sets, not sequences</h3>\n",
    "<ul>\n",
    "    <li>Self-attention is a <em>set</em>-processing mechanism, not a <em>sequence</em>-processing mechanism.\n",
    "        <ul>\n",
    "            <li>We did not feed in an input sentence $\\v{x}_{(1)}, \\v{x}_{(2)}, \\ldots, \\v{x}_{(\\mathit{max\\_length})}$ one word at a time.</li>\n",
    "            <li>We treated the input sentence as a matrix, where each row of the matrix is one element of the input sequence.\n",
    "                And the whole matrix is fed in as a single input in one go.\n",
    "            </li>\n",
    "            <li>This is great for parallelization (although we should acknowledge that transformers are quite expensive to train, even when parallelized).</li>\n",
    "            <li>But it means that the input is now a set (a set of rows), not a sequence!\n",
    "                <ul>\n",
    "                    <li>You could shuffle the rows and it will make no difference: you will calculate the same pairwise similarities and the same context-aware representations.\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We don't want this! We have lost word ordering. It makes ``Dogs hate cats'' no different from ``Cats hate dogs'' &mdash; both are sets (matrices) containing the word embeddings for ``cats'', ``dogs'' and ``hates''.\n",
    "    </li>\n",
    "    <li>Transformers need an extra mechanism to re-inject word ordering information.\n",
    "        <table>\n",
    "            <tr><th></th><th>Word order awareness</th><th>Context-awareness</th></tr>\n",
    "            <tr><th>Bag-of-words (unigrams)</th><td>No</td><td>No</td></tr>\n",
    "            <tr><th>Bag-of-words (bigrams)</th><td>Very limited</td><td>No</td></tr>\n",
    "            <tr><th>RNN</th><td>Yes</td><td>No</td></tr>\n",
    "            <tr><th>Self-attention</th><td>No</td><td>Yes</td></tr>\n",
    "            <tr><th>Transformer</th><td>Yes</td><td>Yes</td></tr>\n",
    "        </table>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c0b87",
   "metadata": {},
   "source": [
    "<h3>Re-injecting word order information</h3>\n",
    "<ul>\n",
    "    <li>To re-inject word order information, we will include the word's position in the input sentence to each word embedding. Let's consider three ways of doing this.</li>\n",
    "    <li>Simple: Concatenate the word position to the word embedding. \n",
    "        <ul>\n",
    "            <li>Concatenate 0 to the word embedding of the first word.</li>\n",
    "            <li>Concatenate 1 to the word embedding of the second word.</li>\n",
    "            <li>And so on.</li>\n",
    "        </ul>\n",
    "        This is not ideal: these integers would dominate the values in the word embeddings, making the word embeddings themselves negligble.\n",
    "    </li>\n",
    "    <li>The original transformer: Add the sine and/or cosine of the position.\n",
    "        <ul>\n",
    "            <li>This means you add values in the range $[-1, 1]$</li>\n",
    "            <li>The values vary cyclically.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Positional embedding: learn an embedding of the position.\n",
    "        <ul>\n",
    "            <li>We know that an embedding layer maps from a word index (i.e. its index in the vocabulary) to a $d$-dimensional vector.</li>\n",
    "            <li>So why not use another embedding layer to map from word positions (i.e. where it is in the sentence) to another $d$-dimensional vector.</li>\n",
    "            <li>Then add the word embedding and the positional embedding.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f8806",
   "metadata": {},
   "source": [
    "<h2>Transformers</h2>\n",
    "<ul>\n",
    "    <li>Transformers put all these ideas together with others. Details unimportant in CS4619.\n",
    "        <ul>\n",
    "            <li>They add some dense layers.</li>\n",
    "            <li>They add some skip links (see the final lecture of AI1 to remind yourself what these are).</li>\n",
    "            <li>They add layer normalization. Layer normalization is similar to, but different from batch normalization. Batch normalization computes means and standard deviatons from all examples in a mini-batch, but this does not work well for sequence data. Instead, layer normalization normalizes each input sequence independently from the others. In other words, it computes means and standard deviations from all inputs in each sequence.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Here is the diagram from the original paper:\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/transformer_encoder.png\" />\n",
    "        </figure>\n",
    "        They used 8 heads and a stack of 6 of these layers. \n",
    "    </li>\n",
    "    <li><b>HERE!! want to simplify non-sentiment analysis part!!!!!!!!!!!!!!!!!!!!!!!</b></li>\n",
    "    <li>In fact, what we have discussed so far (and the diagram) is just the transformer encoder. You'll then want more layers to do your task.\n",
    "        <ul>\n",
    "            <li>E.g. for sentiment analysis (negative, neural, positive), you want a final dense layer containing three neurons as your output layer.</li>\n",
    "            <li>E.g. for a word-level language model, you want a final dense layer containing one neuron per word in your vocabulary as your output layer.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>If you want to do sequence-to-sequence tasks, such as Machine Translation, code completion or question-answering, then you will want to connect your transformer encoder to a transformer decoder.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5453ad",
   "metadata": {},
   "source": [
    "<h2>Training and Pretraining</h2>\n",
    "<ul>\n",
    "    <li>Your chances of training a transformer from scratch are small.</li>\n",
    "    <li>Instead, they are trained by resource-rich organizations on large datasets for next-word prediction (self-supervised learning).</li>\n",
    "    <li>Then, these pretrained transformers are reused and fine-tuned for more specific tasks using a labeled dataset.</li>\n",
    "    <li><b>HERE want pretrained encoders.....!!!!!!</b></li>\n",
    "    <li>Famous examples of popular large-scale language models:\n",
    "        <ul>\n",
    "            <li>GPT: Generative Pretrained Transformer, developed by OpenAI\n",
    "                <ul>\n",
    "                    <li>GPT has gone through several versions, GPT-1, GPT-2, GPT-3</li>\n",
    "                    <li>One difference is expectations about fine-tuning. It is claimed that, to varying degrees, they do not need fine-tuning (lookup discussions of zero-shot, one-shot or few-shot task transfer if you're interested).</li>\n",
    "                    <li>A final difference is that GPT-3 uses sparse attention, which means that attention weights are not computed for all pairs, just for a subset.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>BERT: Bidirectional Encode Representations from Transformers, developed by Google\n",
    "                <ul>\n",
    "                    <li>GPT uses self-attention but only from preceding words, whereas BERT uses self-attenton from preceding and succeeding words.</li>\n",
    "                    <li>Instead of next-word prediction, BERT is pretrained on predicting masked words, which might occur anywhere in the input.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>BART: Bidirectional and Auto-Regressive Transformers, developed by Facebook/Meta\n",
    "                <ul>\n",
    "                    <li>GPT's speciality is generating text; BERT's is classifying text. BART generalises them to accomplish both tasks.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "        &hellip; and dozens more!\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5597db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56496cd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PositionEmbedding\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RMSprop\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_nlp'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.layers import add\n",
    "\n",
    "import keras\n",
    "from keras_nlp.layers import PositionEmbedding\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "from tensorflow import convert_to_tensor, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c25d749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/dataset_5000_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24cbbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "split_point = int(len(df) * 0.8)\n",
    "dev_X = df[\"review\"][:split_point]\n",
    "test_X = df[\"review\"][split_point:] \n",
    "\n",
    "# Target values, encoded and converted to a 1D numpy array\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df[\"sentiment\"])\n",
    "dev_y = label_encoder.transform(df[\"sentiment\"][:split_point])\n",
    "test_y = label_encoder.transform(df[\"sentiment\"][split_point:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eed736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 20000\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "812a9dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 09:30:54.023284: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Create the count vectorization layer, and call adapt on the text-only dataset to create the vocabulary.\n",
    "vectorization_layer = TextVectorization(output_mode=\"int\", max_tokens=max_tokens, output_sequence_length=max_length)\n",
    "vectorization_layer.adapt(convert_to_tensor(dev_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93e72ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vectorization_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc8d2535",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a26f037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughout the code, we will use 100-dimensional word embeddings (including the pretrained GloVe embeddings later)\n",
    "embedding_dimension = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "549265d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the GloVe word embeddings file: produces a dictionary from words to their vectors\n",
    "\n",
    "path = \"../datasets/glove.6B.100d.txt\" # Edit this to point to your copy of the file\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(path)\n",
    "for line in f:\n",
    "    word, coefs = line.split(maxsplit=1)\n",
    "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f2eb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix that associates the words that we obtained from the IMDB reviews earlier \n",
    "# (in the vocabulary) with their GloVe word embeddings\n",
    "\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dimension)) \n",
    "for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "        word_embedding = embeddings_index.get(word)\n",
    "        if word_embedding is not None:\n",
    "            embedding_matrix[i] = word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f43386",
   "metadata": {},
   "source": [
    "The code above comes before Acks.\n",
    "\n",
    "Now new code. for here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72c3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see maybe: https://exchange.scale.com/public/blogs/how-to-build-a-transformer-for-supervised-classification\n",
    "\n",
    "# for Acks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "061cc05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([200])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = convert_to_tensor(range(0, max_length)) # want 200, 100 or ?? \n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d8de9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.arange(0, max_length)\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e9ffbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(1,), dtype=string)\n",
    "x = vectorization_layer(inputs)\n",
    "x = Embedding(input_dim=max_tokens, output_dim=embedding_dimension,\n",
    "            embeddings_initializer=Constant(embedding_matrix), trainable=False)(x)\n",
    "\"\"\"\n",
    "positional_embedding = \\\n",
    "    Embedding(input_dim=max_length, output_dim=embedding_dimension)(\n",
    "        convert_to_tensor(range(0, max_length)))\n",
    "x = add([x, positional_embedding])\n",
    "\"\"\"\n",
    "positional_embedding = \\\n",
    "    Embedding(input_dim=max_length, output_dim=embedding_dimension)(\n",
    "        np.arange(0, max_length))\n",
    "x = add([x, positional_embedding])\n",
    "residual = x\n",
    "x = MultiHeadAttention(num_heads=2, key_dim=64)(x, x)\n",
    "x = add([x, residual])\n",
    "x = LayerNormalization()(x)\n",
    "residual = x\n",
    "x = Dense(100, activation=\"relu\")(x)\n",
    "x = add([x, residual])\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "transformer = Model(inputs, outputs)\n",
    "\n",
    "transformer.compile(optimizer=RMSprop(learning_rate=0.0001), loss=\"binary_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e450583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, 200)         0           ['input_13[0][0]']               \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " embedding_20 (Embedding)       (None, 200, 100)     2000000     ['text_vectorization[11][0]']    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (200, 200, 100)      0           ['embedding_20[0][0]']           \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (200, 200, 100)     51684       ['add_15[0][0]',                 \n",
      " eadAttention)                                                    'add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (200, 200, 100)      0           ['multi_head_attention_6[0][0]', \n",
      "                                                                  'add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (200, 200, 100)     200         ['add_16[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (200, 200, 100)      10100       ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (200, 200, 100)      0           ['dense_15[0][0]',               \n",
      "                                                                  'layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3 (Gl  (200, 100)          0           ['add_17[0][0]']                 \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (200, 64)            6464        ['global_average_pooling1d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (200, 1)             65          ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,068,513\n",
      "Trainable params: 68,513\n",
      "Non-trainable params: 2,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15c48604",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/binary_crossentropy/logistic_loss/mul/Mul' defined at (most recent call last):\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/tw/3vtw0d913p91kxjws16mgkym0000gn/T/ipykernel_47274/2610894403.py\", line 1, in <cell line: 1>\n      transformer_history = transformer.fit(dev_X, dev_y, epochs=10, batch_size=32, validation_split=0.25,\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\", line 893, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 537, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 590, in _compute_gradients\n      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 471, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/binary_crossentropy/logistic_loss/mul/Mul'\nIncompatible shapes: [32,1] vs. [200,1]\n\t [[{{node gradient_tape/binary_crossentropy/logistic_loss/mul/Mul}}]] [Op:__inference_train_function_15506]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer_history \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/binary_crossentropy/logistic_loss/mul/Mul' defined at (most recent call last):\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/tw/3vtw0d913p91kxjws16mgkym0000gn/T/ipykernel_47274/2610894403.py\", line 1, in <cell line: 1>\n      transformer_history = transformer.fit(dev_X, dev_y, epochs=10, batch_size=32, validation_split=0.25,\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/training.py\", line 893, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 537, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 590, in _compute_gradients\n      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 471, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/binary_crossentropy/logistic_loss/mul/Mul'\nIncompatible shapes: [32,1] vs. [200,1]\n\t [[{{node gradient_tape/binary_crossentropy/logistic_loss/mul/Mul}}]] [Op:__inference_train_function_15506]"
     ]
    }
   ],
   "source": [
    "transformer_history = transformer.fit(dev_X, dev_y, epochs=10, batch_size=32, validation_split=0.25, \n",
    "                          callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2)], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bed79b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(transformer_history.history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f052d",
   "metadata": {},
   "source": [
    "<h2>Hugging Face</h2>\n",
    "<p><b>HERE!!!!! Better would be to do sentiment analysis on my IMDB dataset!!!!!</b></p>\n",
    "<ul>\n",
    "    <li>Hugging Face is an AI company: <a href=\"https://huggingface.co/\">https://huggingface.co/</a></li>\n",
    "    <li>They make pretrained models available via their <code>transformers</code> library:\n",
    "        <a href=\"https://huggingface.co/transformers/v3.3.1/pretrained_models.html\">https://huggingface.co/transformers/v3.3.1/pretrained_models.html</a>\n",
    "    </li>\n",
    "    <li>See their installation instructions, but at the time of writing the instructions are:\n",
    "        <ol>\n",
    "            <li>Activate your virtual environment.</li>\n",
    "            <li>Install: <code>pip install transformers</code></li>\n",
    "            <li>Check: <code>python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\"</code><br />\n",
    "                It should print: <code>[{'label': 'POSITIVE', 'score': 0.9998704791069031}]</code></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3acd26c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760917f8ab4d4fc1875179f46a51dd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bee13ca49047ea81e9cefa70395640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 15:40:21.752376: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a14899eb3bb433ca9ceab4138c42997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9962074fe646d29a744b3c80a8ca79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60de5a14cc24b32898921820cefeac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce082d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Opinions about Derek's AI modules vary from one party to another. There is generally disagreement over whether the human element to Derek's ability to learn AI is due to his ability to recognize others more\"},\n",
       " {'generated_text': \"Opinions about Derek's AI modules vary from country to country, but the team has decided to focus on the high end and have a minimum of six hours per day devoted to this mission.\\n\\n\"},\n",
       " {'generated_text': \"Opinions about Derek's AI modules vary from person to person, but the gist of his blog post is that the data provided indicates that Derek has made some sort of major performance improvement over his previous\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(123)\n",
    "\n",
    "generator(\"Opinions about Derek's AI modules vary from\", max_length=40, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70d800",
   "metadata": {},
   "source": [
    "<h2>Final Remarks</h2>\n",
    "<ul>\n",
    "    <li>What about applying transformers to image processing?\n",
    "        <ul>\n",
    "            <li>On large datasets, there are some promising results, especially considering robustness (in other words, performance on unseen out-of-distribution examples). See the ViT (Vision Transformer) architecture: <a href=\"https://arxiv.org/abs/2010.11929\">https://arxiv.org/abs/2010.11929</a></li>\n",
    "            <li>But this paper suggests that it is other aspects of the transformer architecture (e.g. its more aggressive downsampling) that contributes most. If these ideas are incorporated into ConvNets, then ConvNet robustness can rival Transformer robustness: <a href=\"https://arxiv.org/abs/2206.03452\">https://arxiv.org/abs/2206.03452</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Meanwhile, researchers are trying transformers wherever they have set-based or sequence-based examples, e.g. recommender systems and reinforcement learning.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c79597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
