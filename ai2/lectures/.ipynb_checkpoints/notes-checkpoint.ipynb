{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da630f3-5c4c-4651-b90a-61bccc2bbe57",
   "metadata": {},
   "source": [
    "<h1>Notes about LMs</h1>\n",
    "to cover encoder/decoder, seq2seq, MT, QA, CodePilot, music, capabilities of ChatGPT, getting it to issue API calls/Wolfram Alpha calls, reasoning/calculation, controversies (hallucination, impersonation, plagiarism), image2text, text2image.\n",
    "One-shot, few-shot and all that stuff.\n",
    "Prompt engineering as a thing and as a job title, some hints about prompts\n",
    "Tuning, RLHF\n",
    "Smaller models/fewer parameters, faster less resource-intensive training\n",
    "Generation in the way I've shown it: repeatedly predict next. Generation by getting a representation and then feeding it into an encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc94e6e-bcb8-486f-9973-a279290f65b4",
   "metadata": {},
   "source": [
    "We did think we'd focus on GPT and just say there's a whole load more models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ca721-9d75-4b71-a538-379222ad5b62",
   "metadata": {},
   "source": [
    "GPT (GPT 1) a stack of 12 transformer modules, large dataset, pre-trained, self-supervised learning (word-level, I guess).\n",
    "Then fine-tune on various language tasks: minor adaptations to network, weights from pretraining, learn weights on new task-specific datasets, I think. Text classification (vague), entailment, similarity, QA. Is it masked? If so, how?\n",
    "\n",
    "GPT2 similar but larger 1.5 billion parameters, good performance on many tasks without any fine-tuning. Called this zero-shot learning. (Maybe misleading.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ef845a-a6a6-4ae8-bbcb-de5522a11475",
   "metadata": {},
   "source": [
    "Chollet does what we've done: transformer in text classifier.\n",
    "Then he does se2seq. Language Models come later.\n",
    "We could do the same. Maybe with a short section at end of Transformer lecture or start of LM lecture.\n",
    "Or seq2se could be in the Appendix.\n",
    "\n",
    "SO far, we have been using a number of AI techniques for text classification. In our case, sentiment analysis of movie reviews.\n",
    "\n",
    "Many of the most successful applications of deep learning to natural language processing today involve sequene-to-sequence models, seq2seq.\n",
    "The model takes in a sequence (a sentence, a paragraph, or even something longer) and outputs a different sequences (a different sentence, paragraph or document).\n",
    "Examples\n",
    "Machine translation: convert a paragraph in a source language into a corresponding paragraph in a target language\n",
    "Text summarization: convert a long document to a shorter one that capture the essence of the original\n",
    "Question-answering: take in a question, output a suitable answer\n",
    "Chatbot: take a prompt and output a reply or, more generally, take in the history of a conversation and output the next contribution to the conversation\n",
    "\n",
    "Sequence-to-sequence models mostly comprise two components: an encode and a decoder.\n",
    "\n",
    "During training, it is given source sequences and their target sequence (inputs and corresponding outputs, e.g. sentences in English, corresponnding sentences in French)\n",
    "for each input sequence, the encoder outputs a vector\n",
    "the decoder receives the vector and then we feed in the words of the target sequence one by one and looking at the vector and the words it has received so far, it must predict the next word. (And, of course, parameters of the model change in back-prop fashion when its predictions are incorrect).\n",
    "\n",
    "During prediction (after training), it is given only the source sequences (inouts), e.g. sentences in English, no target sequence now, of course\n",
    "for each input sequence, the encode outputs a vector\n",
    "the decode receive the vector and then, based on the vector and the words it has generated so far, it repeatedly predicts the next word.\n",
    "\n",
    "This can be done with all sorts of models. We could use a RNN for both encoder and decoder: picture.\n",
    "\n",
    "Or we could use a transformer. Picture from the paper.\n",
    "\n",
    "Well, no! With an RNN, yes. A single vector. We have the usual problem of RNNs, they progressively forget about what came earlier. so the vector may capture information from the end of the input better than information from the begiinning. BUt also the essence of the input must be captured entiely in the vector passed from the encoder to the decoder. So picture of this.\n",
    "\n",
    "Transformers are much better suited to seq2seq tasks. \n",
    "The encoer (which we've studied) passes not one vector but one per word in the input: a matrix of context-aware vectors.\n",
    "\n",
    "The decoder has either the words in the taregt so far (training) or the words it has predicted so far (prediction) and must preduct the next word  but it has all the vectirs from the deocder. This is not self-attention: the query is the target and the key and values are the outputs of the encoder. And it can use attention so it can see how source and taregt are related. Ugh. Pictire\n",
    "\n",
    "A nuance. AN RNN decoder works one word at a time and so when generating word i will onlu have access to words up to i-1 of the target sequence. But a transformer takes in everything all at once, so it has received the entire target. During training, when generating word i, it will hjave word i of the  target, son it will likely learn to just copy it, whihc will give the correct answer. So, during training, we use causal padding (padding? masking?), which masks the upper half of the attention matrux to prevent infomration about the future pats of the targt being used. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba330d-406e-4dfd-80cf-b7c27c71f448",
   "metadata": {},
   "source": [
    "The second half of the LM lecture can be about NLG (the Appendix is about seq2seq).\n",
    "We see how to use neural networks to generate text. In fact, much the same ideas can be used to generate any sequence data: speech, music, brushstrokes. You need datasets of sequences that you can learn from. Indeed, this is what we have. We start with LMs and how to learn them. So we just need to have a clearer section title that moves us from LMs to NLG, rather than just tacking NLG onto the LM code. And before doing so, I must make the point that the model we show is RNN but today transformer is even more likely.\n",
    "\n",
    "So the way to do NLG now is to train a model (a RNN or transformer) to predict the next word using the pervious words. Then sample from it: feed it an initial string of words (the conditioning data), ask it to generate the next word, add the generated output back to the input data (the conditioning data) and repeat. (Variants, of course)\n",
    "\n",
    "Greedy: next most probable. But produces predictable strings and is repetitive.\n",
    "\n",
    "Stochastic sampling: based on the probabilities. if a word has a probability of 0.1, you choose it about 10% of the time. Now outout is more interesting, more creative (!?!)\n",
    "\n",
    "We can control the stochasticity with a temperature: reweights the probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1193587-1d54-4298-b702-096e9c42d928",
   "metadata": {},
   "source": [
    "So, we're thinking about LM\n",
    "Imp using RNN\n",
    "Imp usng transformer decoder (mentioned, evne if not imp'd) and contrasted with transformer encoder) (This is what Raschka does)\n",
    "Then NLG\n",
    "sampling of various kinds\n",
    "Appendix on seq2seq\n",
    "Appendix on multi modal generation\n",
    "\n",
    "Then next lecture is LLMs inc prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70760e3-7af4-4f59-b6ea-f34797b293eb",
   "metadata": {},
   "source": [
    "Using a trasnformer instead of a RNN. \n",
    "\n",
    "The LM code has examples all the same length, and they overlap. This is not great. It means we can only start making predictions when we have seen max length words. (Not really true of the code.) Treating overlapping samples as independent means relearning patterns. (This all seems abstuse but it motivates seq2seq.)\n",
    "\n",
    "So seq2seq: feed sequences  and predict next word at each point. This means one example is like many overlapping examples.\n",
    "\n",
    "I'm confused cos he then says thatfor simple NLG we only need the decoder. There is no source sequence.\n",
    "\n",
    "He presents a transformer for it. He has presented seq2seq after text classification, developing a transformer encoder and decosder. Then for NLG he used the decoder. In this treatment seq2seq comes before LMs. \n",
    "\n",
    "So GPT-3 is a stack of transformer decoders and a big training corpus.\n",
    "\n",
    "All it does is sample from a LM, i.e. from a statistcal model of which words come after which words.\n",
    "\n",
    "According to speech act theory, language is used to carry out actions: apologizing, promising, ordering, answering, requesting, complaining, warning, inviting, refusing, and congratulating. Thes eare all examples of speech acts. But they depend on having certain beliefs, desires and intentions. If I am to sincerely apologise to you, then I must believe that I have wronged you, desire good relations with you, and intend to restore our good rleations (or something like that!). Think about the beliefs, desires and intentions tha underlie acts like promising, complaining and so on. Since systems built atop of language models have no beliefs, desires and intentions, any language they produce that resembles a speech act is not really the performance of that act. They may say \"I'm sorry that I offended you'' but this is not truly the speech act of apologizing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e4d0b4-f7a4-46bb-a7d1-e53646d6dcec",
   "metadata": {},
   "source": [
    "WHat of other generative AI? An appendix? Yu can generate other things, code, images and videos being common.\n",
    "generative task oriented models such as Gato, the prompt can be extremely high level and define a task you need help with (“I need to organize a one week trip to Greece”).\n",
    "\n",
    "Code: just like the LMs above bu trained on code. (Or in the case of GTP it seems to do it anyway, because trained on lots of code anyway.)\n",
    "Estaimets that >90% of professional programmers now turn to these tools. Issues are the ones I've said but also dip in traffic to StackOverflow. Willwe face problems with resh problems in the future?\n",
    "\n",
    "AWS CodeWhisperer is a GitHub Copilot alternative that actually tracks the references used to generate code. \n",
    "It will even tell you the license and GitHub repo so you can investigate further https://aws.amazon.com/codewhisperer/\n",
    "\n",
    "(Somewhere will want ot talk about pollution of the training data. Suppose lots of AI-generated text, code, images, videos, audio get published on the Internet. Next time we train, we train on these too. What are the consequences of this? )\n",
    "\n",
    "Techniques: GANs, Variational Encoders, diffusion models.\n",
    "\n",
    "Examples: Google's Deep Dream, midflow(?), Runway, DALLE-2 or Stable Diffusion A different list:  image generators such as DALL-E, Midjourney, and Stable Diffusion. Now photo-realistic\n",
    "\n",
    "Multimodal examples: from text to image, from image to text (captioning), from image to image (aplying styles).\n",
    "Playlist captioning.\n",
    "Midjourney text to images\n",
    "Runway doing text to video and image to video and others: https://runwayml.com/\n",
    "\n",
    "Meta has released AUdioCraft tools. ot quite open source but free use by researchers but restrcitons on commercial use. Includes MusicGen can generate music promoted by text or melody. AudioGen which generates, e.g. sound effect frm text prompts., e..g\"dog barking\".\n",
    "\n",
    "Google various tools inc text to music: https://google-research.github.io/seanet/musiclm/examples/?s=03\n",
    "\n",
    "Diffusion models have become the new state-of-the-art in image generation, clearly pushing aside the previous approaches such as GANs (Generative Adversarial Networks). It is important to note, though, that the diffusion mechanism is not dependent on the Transformer architecture. However, most modern diffusion approaches do include a Transformer backbone ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db234cc-ae63-4084-8a8f-5dd75cee644e",
   "metadata": {},
   "source": [
    "So for a decoder using a transformer to learn a LM. There ia a nuance (don't worry too much about it) due to feediing in the whole sentence in one go. The first multi-head attention block is masked (see 16.6 p553 of Raschka. Maybe even make sure if you include  diagram of the decoder that this is clear.\n",
    "\n",
    "You know we feed in the whole sentence as a matrix, one row per word, rather than feeding it in word by word. But during training, when predicting word i, we should only look at words up to i; we are cheating if we lok at words from i. We must hide those words, which we do with masking.\n",
    "\n",
    "So the words are all fed in at the bottom; the next block masks the later words for each word. Then we may do further multi-head attention. And then a dense layer with a softmax on its outouts to predict the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc77cb7-b58d-4d64-b092-ed73af36eeac",
   "metadata": {},
   "source": [
    "So then NLG and appendices on seq2seq (inc full trasnforme r diagram with link between encoder and decoder), and other forms of generation. Find lots of links to illustrate those so we can end with some fun. E.g. is this a person web site. \n",
    "\n",
    "Note although seq2seq and Generative AI may be appendices, it makes sense to have section headings in the main part to refer to them. (It may even be that Generative AI is just in the main part, after all, it has no detail)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520a80a-25f7-49b7-80be-d1173e08efb5",
   "metadata": {},
   "source": [
    "LLMs\n",
    "\n",
    "So now LLMs: deep, lits of data. Some differences accorsing ot he models used (e.g. decoder only, or encoder and decoder). Also not always ptrained to predict the next word, might be trained to predict various missing words.\n",
    "\n",
    "Often pre-trained using self-supervised learning (can they predict next/missing words) from lots of text. (To confuse matters, some people refer to this as unsupervised or unsupervised pre-training but i'm going to stick to saying self-supervised pre-training) Then fine-tuned (is that my words from AI1, transfer learning: transfer and fine-tune after extending with a few more layers especially dense layers and a new output layer) on more specific tasks - although not always, since some seem to perform well on various tasks without needing more training. (And RLHF as a different way to fine-tune instead of training on a labeled training set.)\n",
    "\n",
    "So many of these it's overwhelming and fast-moving.\n",
    "My focus in on GPT family.\n",
    "\n",
    "Gneerative Pre-Trained Transformer (GPT) family of LMs from OpenAI. All (?) use just a transformer decode, no encoder.\n",
    "\n",
    "Use the table p.563 of Raschla for release years and number od params for GTP1-3.\n",
    "\n",
    "GPT-1. Transformer decoder. Pre-train. Trained to predict next word. If all you want is a LM, a next-word predictor, something for sto chastic NLG, you're done. But for other tasks, we need fine-tuning.\n",
    "\n",
    "E.g. for text classification, add a dense ouptut layer and train on some labeled examples.\n",
    "\n",
    "For some other tasks, you need to add not only an outut layer but a bit of extra preprocessing. (Is it necessary to say this as part of the model. Why not just explain how to prepare the examples for these tasks.) Tasks include: entailment (examples of form Start Premise Delim Hypothesis Extract); similarity (Start Text1 Delim Text2 Extract twice and added); mutlipke choice (Start Context Delim Answer1, Extract and more example for the other answers).\n",
    "\n",
    "What if you try tasks without any fine-tuning. These are zero-shot tasks (Some people are calling it zero shot learning, and I think is confusing with something else). It does surprisingly well on some of these. (what exactly?) This encouraging result inspired GPT-2 which tries to get away from the need for task-specific fine-tuning building a more general model.\n",
    "\n",
    "I wonder if zero-shot, one-shot and few-shot prompting is the way to phrase it.\n",
    "\n",
    "GPT-2. We don't want to do any fine-tuning with extra layers and preprocessing. We wantto train it once and for all, and then use it for multiple different tasks. It requires that you feed it with a context or prompt that acts a a hint for what task it performs. (This makes it sound slike it uses an if to choose what code to run.) So utput probs are conditioned on input and task type p(output | input, task). So if context is translate to French, My aunt's pen is on the table, La plume de ma tante est sur la table (presumbaly a traning exmaple, but how do you get these in this form?) At preidction time, thwre would be no FRench part. So there is some lack of clarity about what this is trained on. Is it the same as notmal? And what it uses at NLG time: do we process inputs/insists on imperatives?\n",
    "\n",
    "GPT-3 retreats from pure zero-shot tasks, where it sees no examples of a task. It uses uses one-shot and few-shot tasks. It includes one or a few examples of each task Again, details seem vague. How does it do this. Are these added to the normal traing data? How found? How prepared? Even more confusing, I'm not sure this is durig training. Maybe it's soemthing the user does at NLG tme? Fig 16-12 in Rascka shows one-shot and few-shot saying no weight/gradien  updates are performed. Othere changes: more parameters, sparse attention which attends to only a subset of the words in the example.\n",
    "\n",
    "Hugging Face\n",
    "\n",
    "Can use GPT2 pip install transformers==4.9.1 maybe and see p567. OK so NLG but what about it being zero-shot??? Is it up to me to formulate translate, X, Y as my inputs?\n",
    "\n",
    "GPT-3.5 GTP-4\n",
    "\n",
    "GPT4: multimodal” model, which means it can accept images as well as text as inputs, allowing users to ask questions about pictures. The new version can handle massive text inputs and can remember and act on more than 20,000 words at once, letting it take an entire novella as a prompt. Available in ChatGPT Plus, the paid version\n",
    "\n",
    "https://openai.com/gpt-4\n",
    "\n",
    "GPT-4 can literally turn a napkin sketch into a fully functional website with code https://t.co/Ktc0Y0vxaG \n",
    "\n",
    "\n",
    "ChatGPT\n",
    "\n",
    "Other LLMs.\n",
    "\n",
    "Don't want to say much.\n",
    "\n",
    "Tranformer catalog: https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/?s=03 (Maybe not all LLMs?)\n",
    "\n",
    "Could mention GPT is decoider, some are encoder, some are encoder-decoder. There is also unidirectional vs bidirectional. BERT is encoder and birecrytional (maybe non-directional is a better word because bidirectional here is different from bidiorectional RNNs. So bidirectionlal here is typically because you are not predicting next word but predicting missing word. So you are allowed to look at preceding and subsequent parts of the sentence. Lots more can be said about BERT but we won't. BART is encoder-decoder and bidirectional.\n",
    "\n",
    "Llama 2 from Meta is \"free\" after the original Llama was leaked. NOw used by many researchers.\n",
    "\n",
    "OpenAI certainly releases tweaks to GPT on a regular basis, and given the regularity with which chief executive Sam Altman talks about AI safety, it’s perfectly plausible that those tweaks are largely safety-focused. And so if the system is getting worse, not better, it’s perhaps because of that trade-off. One roblem here is a lack of transparency: we don't know what's changing. little explanation, and no ability for users to understand why or how each new model differs.\n",
    "\n",
    "\n",
    "Anthropic's Claude 2 a ChatGPT rival (is it a LM or a chat?). can summarise blocks of text of up to 75,000 words, broadly similar to Sally Rooney’s Normal People. In chat, it is just as prone to hallucinations.\n",
    "\n",
    "Tuning: RLHF, Lora\n",
    "\n",
    "See Google's leaked document, esp timeline at end for how the field evolved in a few months using open source models and smaller models requiring lowe cot hardwar and lots of innovation: https://www.semianalysis.com/p/google-we-have-no-moat-and-neither\n",
    "\n",
    "Applications\n",
    "CrossingMinds launced GPT Spotlight, ChatGPT tech built for e-commerce stores to enable conversational product search and discovery.\n",
    "\n",
    "OPenAI's API for ChatGPT means that others can build apps that use it. Just like any site that has anything to do with georgraphical locations (hotel, restuaants) embed Google's Maps Embed API, most web sites will embed a chatbot soon for customer service. See above CrossingMinds.\n",
    "\n",
    "DuoLingo, Khan academy, customer servcice e.g. Stripe\n",
    "\n",
    "OpenAI researchers came up with the idea of training GPT to follow human instructions . The resulting models are called InstructGPT. The authors did this by using a small amount of human-labeled data from a large variety of tasks to further train GPT. As before, this is a \"fine-tuning\" process, but the resulting Instruct GPT model is capable of doing a wide range of tasks, and is, in fact, the class of models used by the popular ChatGPT engine. Since these models can accomplish a myriad of tasks, we refer to them as foundation models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756b29b-6659-4f12-a13f-b9585cd37e46",
   "metadata": {},
   "source": [
    "Capabilities\n",
    "\n",
    "autocomplete on steroids\n",
    "\n",
    "They can't reason.\n",
    "\n",
    "But see this thread that describes a model trainied on reasoning prblems (texts) and asked to do step by step and otger tricks to get it to do stuff: https://twitter.com/alewkowycz/status/1542559176483823622?t=acrsTJ67VSQT3ObqQr7DGw&s=09\n",
    "\n",
    "But see this paper, which evalauted GPT-3 on several tasks involving analogical reasoning and shows performance that is competitive with humans:\n",
    "Taylor Webb and Keith J. Holyoak and Hongjing Lu: Emergent Analogical Reasoning in Large Language Models, \n",
    "https://doi.org/10.48550/arXiv.2212.09196\n",
    "It has some concluions about limitations esp physical knowledge and other...\n",
    "It seems to me that a major limitation is that they required the system to select from among candidate answers rather than to generate answers.\n",
    "\n",
    "And see the use of tools: “Toolformer: Language Models Can Teach Themselves to Use Tools” https://arxiv.org/abs/2302.04761 : teach an LLM to use tools, like a calculator or search engine, in a *self-supervised manner*\n",
    "And Wolfram Alpha: https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/?s=03\n",
    "And LLMAugmenter: https://arxiv.org/pdf/2302.12813.pdf\n",
    "Augmented Language Models: a Survey: https://arxiv.org/abs/2302.07842\n",
    "\n",
    "But take a problem such as wolf, goat, cabbages: it can solve. But then say, there's plenty of room in the boat for all of them. It solves the old way.\n",
    "\n",
    "What of LLM and search. Breaks the contract: at least as accurate as the web!\n",
    "But also argumentative. Bing + LLM: https://t.co/X32vopXxQG \n",
    "argues with a user, gaslights them about the current year being 2022, says their phone might have a virus, and says \"You have not been a good user\" More: https://dkb.blog/p/bing-ai-cant-be-trusted?s=03\n",
    "\n",
    "And Google's LLamda plus search was disastrous. The demo video had a wrong answer: What new discoveries from the James Webb space telescope (JWST) can I tell my nine-year old about?”\n",
    "Bard’s response – in a video demo posted online – includes an answer suggesting the JWST was used to take the very first pictures of a planet outside the Earth’s solar system, or exoplanets. (the first image was instead done by Chauvin et al. (2004) with the VLT/NACO using adaptive optics.) nvestors wiped more than $100bn (£82bn) off the value of the search engine’s parent company, Alphabet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7315b514-8dfc-49fe-953a-e635df9795ab",
   "metadata": {},
   "source": [
    "Problems\n",
    "\n",
    "Hallucination\n",
    "People are investigating solutions. ONe can imagine fact-checking. Or even ensembles: produce mutliple responses from the same LLM or from several LLMs, extract facts and combine/vote (somehow)\n",
    "\n",
    "A lawyer used ChatGPT to do \"legal research\" and cited a number of nonexistent cases in a filing, and is now in a lot of trouble with the judge\n",
    "\n",
    "A Large Language Model trained on scientific papers.\n",
    "Type a text and http://galactica.ai will generate a paper with relevant references, formulas, and everything. Removed within days.\n",
    "\n",
    "ChatGPT with tools (using APIs) might be a new mechanism for creating cybersecurity attacks. E.g. https://simonwillison.net/2023/Apr/14/worst-that-can-happen/#data-exfiltration\n",
    "\n",
    "Pollution of training set.\n",
    "\n",
    "Special problems in education. \"Apparently they now must submit homework via Google Docs so the teacher can view the history to see if they really wrote it or not.\" You can think of ways students will circumvent this: get the whole essay; type it in in bits with errors; type the rest later and fix some of the errors; or maybe ask ChatGPT for drafts and improved versions.\n",
    "\n",
    "+ve Homework sites such as chegg.com and essay mills have seen a drop in visitors.\n",
    "\n",
    "That ChatGPT seems so easily to write grade A essays, he suggests, “is mainly a comment on what we value. Students regurgiatte and may be little more than human bullshitters like ChatGPT is an AI bullshitter. And we reward for surface-level correctness, ignoring understanding and criticla thinking.\n",
    "\n",
    "Using them to discuss communication, citing. Teaching people how to use them effectibely for future life, a life-skill.\n",
    "\n",
    "More generally, more invigilated assessment (class tests, university writte exams), more oral examination, assessment with versioning, higher standards!\n",
    "\n",
    "DueLinogo, Jhan acadmy.\n",
    "\n",
    "Some profs requring their use, resposible and skeptical use.\n",
    "\n",
    "Note how OpenAI shut down its classifier. I saw something that said watermarking was impossible!\n",
    "\n",
    "But GPTZero is a company that aims to provide detection tools. One allows replay of edits history, so you watch a little movie of the document changing. Perhaps their are edit hisotiers that are signatures of AI assistance. E.g. abrupt changes. Of course, any tool the teacher uses, the student can use until their submission passes that test. And someone will produce the counter-tool.\n",
    "\n",
    "Maybe do programming as a special case too. CoPilot, ChatGPT, website from sketches, multi-language, the issues in my talk, StackOverflow banning AI_generated posts, but also traffic way down but if AI east itself, then how will we gte fresh solutions to problems that crop up).\n",
    "\n",
    "For coding, there are independent products (like OpenAI Codex, ChatGPT, Google Bard) or products that are natively integrated inside existing platforms (like GitHub Copilot, Replit Ghostwriter, Amazon CodeWhisperer).\n",
    "\n",
    "A Purdue Uni study arXiv:2308.02312v2 Who Answers It Better?... Says ChatGPT gets 50% wrong (check this caim!)\n",
    "\n",
    "Copilot bugs fixing The completion will give you suggestions for potential bugs in your code https://t.co/QBd7NtpKWM\n",
    "\n",
    "Very interesting research on the impact of Copilot on developer productivity https://t.co/Yr92ZlUfQb (https://twitter.com/xamat/status/1568010035112067073?t=2VNayiuCkIrojeoyC-DqDQ&s=03)\n",
    "\n",
    "StackOverflow traffic down 14%. Some suggetsion they will charge large Ai companies that scrape their site for training data (not clear how). And StackOverflow still has things going for it:  1) providing high accuracy / high reliability answers to more complex questions that language models might not have the capability to answer, and 2) providing answers to questions in new technologies / problem spaces that the models have not had previous data to train on.Also they are proposing OverflowAI as a response: conversational interface to StackOverflow from VSCode.\n",
    "\n",
    "Copyright\n",
    "Image generators have raised serious ethical concerns around artistic ownership and copyright, with evidence that some AI programs have being trained on millions of online images without permission or payment, leading to class action lawsuits.\n",
    "\n",
    "Tools have been developed to protect artistic works from being used by AI, such as Glaze, which uses a cloaking technique that prevents an image generator from accurately being able to replicate the style in an artwork.\n",
    "\n",
    "Photo-realsitic but fake: Trump's arrest, Pope in a puffer jacket\n",
    "\n",
    "Voice-cloning allows impersonatin, scamming https://elevenlabs.io even your own voice\n",
    "\n",
    "Writing stgudent essays, academic papers, reviews o academic papers,... But plagued with hallucinations,. e.g. faje citations.\n",
    "\n",
    "Text to video\n",
    "\n",
    "2D to 3D\n",
    "\n",
    "\"Habsburg AI, referring to an infamously inbred European royal dynasty. Habsburg AI is a system that is so heavily trained on the outputs of other generative AIs that it becomes an inbred mutant, replete with exaggerated, grotesque features.\"\n",
    "The Curse of Recursion: Training on Generated Data Makes Models Forget, https://arxiv.org/abs/2305.17493v2\n",
    " research suggests that large language models, like the one that powers ChatGPT, quickly collapse when the data they are trained on is created by other AIs instead of original material from humans\n",
    " Self-Consuming Generative Models Go MAD: https://arxiv.org/abs/2307.01850\n",
    " without fresh data, an autophagous loop is created, doomed to a progressive decline in the quality of content\n",
    " Ross Anderson, professor of security engineering at Cambridge University and the University of Edinburgh, wrote in a blog post discussing the paper: “Just as we’ve strewn the oceans with plastic trash and filled the atmosphere with carbon dioxide, so we’re about to fill the Internet with blah.\n",
    "\n",
    " Empahsising its mysteerious nature: https://www.vice.com/en/article/epzyva/ai-chatgpt-tokens-words-break-reddit Reddit usernames like ‘SolidGoldMagikarp’ and other weird tokens are somehow causing the chatbot to give bizarre responses.\n",
    "\n",
    " Plagiarism, e.g. CNET artciles close to others: https://futurism.com/cnet-ai-plagiarism\n",
    "\n",
    " See Denning in CACMJune 2023 for a clean treatment of some of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad755723-f5ca-444a-998b-72e3479c8ca9",
   "metadata": {},
   "source": [
    "Prompt engineering\n",
    "\n",
    "Xavi: https://amatriain.net/blog/PromptEngineering and https://amatriain.net/blog/prompt201\n",
    "\n",
    "At one level, this is just how to write good prompts. But at another level it is how to write programs that have templates that tyou can instaniate frorm a datbase, to write lots of promots.\n",
    "\n",
    "Suppose I have a dictionary of lecturers and, for each, their three best research publications, \n",
    "e.g. {\"Derek Bridge\": [..., ..., ...], \"Ken Brown\": [..., ..., ....]}\n",
    "for lecturer, publications in dictionary.items():\n",
    "    publicatons_str = \"' \".join(publications)\n",
    "    prompt = \"Write a two paragraph long description of {}'s research interests, given that he has published papers on \".format(lecturer, publications)\"\n",
    "    callGPT(prompt)\n",
    "\n",
    "A wiredness compared with normal programming is the result is stochastic: differemnt every time.\n",
    "\n",
    "Then rules-of-thum, including\n",
    "Chain of thought prompting, we explicitly encourage the model to be factual/correct by forcing it to follow a series of steps. Example of this kind of prompt:\n",
    "“What European soccer team won the Champions League the year Barcelona hosted the Olympic games?\n",
    "\n",
    "Use this format:\n",
    "\n",
    "Q: A: Let’s think step by step. Therefore, the answer is .”\n",
    "Or you might even give an example where you do some reasoning step by step.\n",
    "\n",
    "Generate different opinions. \n",
    "“The text between <begin> and <end> is an example article.\n",
    "\n",
    "<begin>....an article presneitn an opinion...<end>\n",
    "Given that example article, write a similar article that disagrees with it. “\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae264efe-1a3a-42cb-8fdd-eed72afaffff",
   "metadata": {},
   "source": [
    "<!--\n",
    "<h1>Pretrained Language Models</h1>\n",
    "<ul>\n",
    "    <li>There has been a lot\n",
    "        of work on producing pretrained models that you can use as layers in your own architecture.\n",
    "    </li>\n",
    "    <li>One famous example is Google's BERT: <a href=\"https://github.com/google-research/bert\">https://github.com/google-research/bert</a>\n",
    "        <ul>\n",
    "            <li>Google claim that this has much improved their search engine's ability to answer\n",
    "                questions (as opposed to traditional keyword search): <a href=\"https://www.blog.google/products/search/search-language-understanding-bert/\">https://www.blog.google/products/search/search-language-understanding-bert/</a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        Another famous example is GPT-3, releaed in 2020. It has 175 billion parameters and produces text that is hard to distinguish from text produced by humans.\n",
    "        <ul>\n",
    "            <li>Consider this, for example: <a href=\"https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3\">https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3</a>\n",
    "            </li>\n",
    "            <li>Or try Dungeons &amp; Dragons game: <a href=\"https://play.aidungeon.io/main/landing\">https://play.aidungeon.io/main/landing</a>\n",
    "                (But note the controversy too: some players were typing words that caused the game to generate stories depicting sexual encounters involving children <a href=\"https://arstechnica.com/gaming/2021/05/it-began-as-an-ai-fueled-dungeon-game-then-it-got-much-darker/\">https://arstechnica.com/gaming/2021/05/it-began-as-an-ai-fueled-dungeon-game-then-it-got-much-darker/</a>)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>These pretrained models use neural network architectures that go beyond what we've seen so far. For\n",
    "        example, they might be bidirectional; they might use one-dimensional convolutional \n",
    "        layers instead of RNNs/LSTMs/GRUs; and they might use the new Transformer architectures.\n",
    "        We'll (briefly) discuss some of these ideas in the next lecture.\n",
    "    </li>\n",
    "</ul>\n",
    "-->\n",
    "<h1>Language Models and their Applications</h1>\n",
    "<b>TWO CELLS TO MODIFY PRIOR TO APPENDIX</b>\n",
    "<ul>\n",
    "    <li>The most successful Language Models use neural network architectures that go beyond what we've seen so far. For\n",
    "        example, they might be bidirectional; and they might use one-dimensional convolutional \n",
    "        layers instead of RNNs/LSTMs/GRUs. Most likely, they use the new Transformer architecture, which we cover\n",
    "  in the next lecture. BERT and GPT, for example, are Language Models that use the Transformer architecture.\n",
    "    </li>\n",
    "    <li>Google claims that BERT (<a href=\"https://github.com/google-research/bert\">https://github.com/google-research/bert</a>) has much improved their search engine's ability to answer\n",
    "                questions (as opposed to traditional keyword search): <a href=\"https://www.blog.google/products/search/search-language-understanding-bert/\">https://www.blog.google/products/search/search-language-understanding-bert/</a>\n",
    "    </li>\n",
    "    <!--<li><i>Sunspring</i> is a sci-fi movie whose script was generated by an LSTM trained on existing\n",
    "        movie scripts: <a href=\"http://www.thereforefilms.com/sunspring.html\">http://www.thereforefilms.com/sunspring.html</a>\n",
    "    </li>-->\n",
    "    <li>GPT-3, released in 2020, has 175 billion parameters and produces text that is hard to distinguish from text produced by humans.\n",
    "        <ul>\n",
    "            <li>Consider this, for example: <a href=\"https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3\">https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3</a>\n",
    "            </li>\n",
    "            <li>There are also theatre pieces that it has been involve in, e.g.: <a href=\"https://www.theguardian.com/stage/2021/aug/24/rise-of-the-robo-drama-young-vic-creates-new-play-using-artificial-intelligence\">https://www.theguardian.com/stage/2021/aug/24/rise-of-the-robo-drama-young-vic-creates-new-play-using-artificial-intelligence</a></li>\n",
    "            <li>Or try Dungeons &amp; Dragons game: <a href=\"https://play.aidungeon.io/main/landing\">https://play.aidungeon.io/main/landing</a>\n",
    "                (But note the controversy too: some players were typing words that caused the game to generate stories depicting sexual encounters involving children <a href=\"https://arstechnica.com/gaming/2021/05/it-began-as-an-ai-fueled-dungeon-game-then-it-got-much-darker/\">https://arstechnica.com/gaming/2021/05/it-began-as-an-ai-fueled-dungeon-game-then-it-got-much-darker/</a>)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Meta's Galactica is Language Model trained on 48 million examples of scientific articles. Meta claimed Galactica “can summarize academic papers, solve math problems, generate Wiki articles, write scientific code, annotate molecules and proteins, and more.” Made available on the 15th November 2022, the public demo was taken down on the 17th November after a storm of criticism. Criticism focused on its inability to distinguish truth from fiction &mdash; which is a skill that a tool to assist scientists ought to have! E.g. <a href=\"https://twitter.com/Michael_J_Black/status/1593133722316189696\">https://twitter.com/Michael_J_Black/status/1593133722316189696</a>.\n",
    "    </li>\n",
    "    <li>These ideas are used to create software that can paraphrase, e.g. <a href=\"https://quillbot.com/\">https://quillbot.com/</a>\n",
    "        <!-- https://twitter.com/mattlodder/status/1488894436801232896?t=nK_Of2cKjrQpZa6-yHgtKA&s=03 -->\n",
    "    </li>\n",
    "    <!--\n",
    "    <li>There are researchers who are trying to generate explanations using these techniques.</li>\n",
    "    <li>But let's look briefly at image captioning, machine translation and question-answering.</li>\n",
    "    -->\n",
    "    <li>The same ideas lie behind Microsoft's <a href=\"https://copilot.github.com/\">Copilot</a>\n",
    "        <ul>\n",
    "            <li>Available as a VSCode Extension, it has been trained on billions of lines of public code.</li>\n",
    "            <li>It's like an autocomplete, but for whole lines of code and even entire functions.</li>\n",
    "            <li>Does it make programmers obsolete?\n",
    "                <ul>\n",
    "                    <li>Since it understands nothing, there are claims that it often writes buggy code, e.g.\n",
    "                        <a href=\"https://twitter.com/asmeurer/status/1410399693025153028\">https://twitter.com/asmeurer/status/1410399693025153028</a>\n",
    "                    </li>\n",
    "                    <li>There are concerns about whether what it has been trained on is truly 'public' and\n",
    "                        whether the code it generates is original enough to escape citation/licence problems.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>OpenAI's Codex is similar to Microsoft's Copilot: <a href=\"https://openai.com/blog/openai-codex/\">https://openai.com/blog/openai-codex/</a> but, instead of code completion, you tell it what you want in English. Take a look &mdash; it's really cool.\n",
    "    </li>\n",
    "    <li>We can generate music in this way too, e.g. <a href=\"https://folkrnn.org/\">https://folkrnn.org/</a></li>\n",
    "    <li>The biggest stir most recently was Open AI's <i>Chat-GPT</i> (<a href=\"https://openai.com/blog/chatgpt/\">https://openai.com/blog/chatgpt/</a>). The demo is <a href=\"https://chat.openai.com/auth/login\">here</a> but sometimes unavailable due to demand. It uses GPT for conversation. But, again, the main problems are: it has no understanding; it cannot reason; it cann distinguish fact from fiction. It just makes shit up.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e522fe-9b94-4864-bbc4-a903039214c6",
   "metadata": {},
   "source": [
    "<h1>A Final Thought</h1> \n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/parrot.png\" />\n",
    "            <figcaption>\n",
    "                Image from <a href=\"https://twitter.com/cuducos\">Cuducos</a>\n",
    "            </figcaption>\n",
    "        </figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762af675-ff96-4eae-8f43-6d5082903873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
